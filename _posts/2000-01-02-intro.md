---
title: "Publication"
bg: blue
color: white
fa-icon: quote-left
---

* Obtaining 3D geometry from images is a well studied problem by the computer vision community. In the concrete case of a single image, a considerable amount of prior knowledge is often required  to obtain plausible reconstructions. Recently, deep neural networks in combination with 3D morphable models (3DMM) have been used in order to address the lack of scene information, leading to more accurate results. Nevertheless, the losses employed during the training process are usually a linear combination of terms where the coefficients, also called hyperparameters, must be carefully tuned for each dataset to obtain satisfactory results. In this work we propose a hyperparameters-free loss that exploits the geometry of the problem for learning 3D reconstruction from a single image. The proposed formulation is not dataset dependent, is robust against very large camera poses and jointly optimizes the shape of the object and the camera pose.*

Find the full paper on [arXiv](https://arxiv.org/abs/1808.09559) or download the PDF directly from [here](https://github.com/imatge-upc/mrl/raw/gh-pages/ramon-2019-bmva.pdf).

If you find this work useful, please consider citing:

<i>
Eduard Ramon, Jordi Villar, Guillermo Ruiz, Thomas Batard and Xavier Giro-i-Nieto. "Temporal Saliency Adaptation in Egocentric Videos", BMVA technical meeting: 3D vision with Deep Learning, 2019.
</i>

<pre>
@inproceedings{ramon2019mrl,
title={Plug-and-Train Loss for Single View 3D Reconstruction},
author={Eduard Ramon, Jordi Villar, Guillermo Ruiz, Thomas Batard and Xavier Giro-i-Nieto},
journal={arXiv preprint arXiv:1808.09559},
year={2019}
}
</pre>



